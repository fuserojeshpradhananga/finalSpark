{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import calendar\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSVToPostgreSQL\").getOrCreate()\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pg_user = os.getenv(\"PG_USER\")\n",
    "pg_password = os.getenv(\"PG_PW\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV then Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"/home/rojesh/Documents/finalSpark/Fuel_Station_Information.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"/home/rojesh/Documents/finalSpark/Hourly_Gasoline_Prices.csv\", header=True, inferSchema=True)\n",
    "\n",
    "joined_df = df1.join(df2, \"Id\", \"inner\")\n",
    "\n",
    "cleaned_df = joined_df.dropna()\n",
    "\n",
    "# Data Cleaning:Removing rows where the \"Type\" column has the value \"autostradle\"\n",
    "cleaned_df = cleaned_df.filter(F.col(\"Type\") != \"Autostradle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the cleaned file as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquet_path = \"../trying/parquet\"\n",
    "cleaned_df.coalesce(6).write.parquet(parquet_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Spark to Dbeaver and writing the cleaned csv into the Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o122.setProperty.\n: java.lang.NullPointerException\n\tat java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)\n\tat java.base/java.util.Properties.put(Properties.java:1340)\n\tat java.base/java.util.Properties.setProperty(Properties.java:230)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/rojesh/Documents/finalSpark/final.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m properties \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: pg_user,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpassword\u001b[39m\u001b[39m\"\u001b[39m: pg_password,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdriver\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39morg.postgresql.Driver\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m table_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnewtable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m cleaned_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mjdbc(url\u001b[39m=\u001b[39;49mjdbc_url, table\u001b[39m=\u001b[39;49mtable_name, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m, properties\u001b[39m=\u001b[39;49mproperties)\n",
      "File \u001b[0;32m~/Documents/finalSpark/final/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1918\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1913\u001b[0m jprop \u001b[39m=\u001b[39m JavaClass(\n\u001b[1;32m   1914\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mjava.util.Properties\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1915\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39m_gateway_client,\n\u001b[1;32m   1916\u001b[0m )()\n\u001b[1;32m   1917\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m properties:\n\u001b[0;32m-> 1918\u001b[0m     jprop\u001b[39m.\u001b[39;49msetProperty(k, properties[k])\n\u001b[1;32m   1919\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39mjdbc(url, table, jprop)\n",
      "File \u001b[0;32m~/Documents/finalSpark/final/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/finalSpark/final/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/finalSpark/final/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o122.setProperty.\n: java.lang.NullPointerException\n\tat java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)\n\tat java.base/java.util.Properties.put(Properties.java:1340)\n\tat java.base/java.util.Properties.setProperty(Properties.java:230)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"newtable\"\n",
    "\n",
    "\n",
    "\n",
    "cleaned_df.write.jdbc(url=jdbc_url, table=table_name, mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the file from Postgres and Working in it for our queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o130.load.\n: java.lang.NullPointerException\n\tat java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)\n\tat java.base/java.util.Properties.put(Properties.java:1340)\n\tat java.base/java.util.Properties.setProperty(Properties.java:230)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$asProperties$1(JDBCOptions.scala:54)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:54)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/rojesh/Documents/finalSpark/final.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#filteringtable\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m filter_condition \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1=1 LIMIT 10000\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m df3 \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mjdbc\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m, jdbc_url) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mdbtable\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m(SELECT * FROM \u001b[39;49m\u001b[39m{\u001b[39;49;00mtable_name\u001b[39m}\u001b[39;49;00m\u001b[39m WHERE \u001b[39;49m\u001b[39m{\u001b[39;49;00mfilter_condition\u001b[39m}\u001b[39;49;00m\u001b[39m) AS filtered_table\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m.\u001b[39;49moptions(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mproperties) \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m row_count \u001b[39m=\u001b[39m df3\u001b[39m.\u001b[39mcount()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rojesh/Documents/finalSpark/final.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of rows read: \u001b[39m\u001b[39m{\u001b[39;00mrow_count\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \n",
      "File \u001b[0;32m~/Documents/finalSpark/final/lib/python3.10/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload())\n",
      "File \u001b[0;32m~/Documents/finalSpark/final/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/finalSpark/final/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/finalSpark/final/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o130.load.\n: java.lang.NullPointerException\n\tat java.base/java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)\n\tat java.base/java.util.Properties.put(Properties.java:1340)\n\tat java.base/java.util.Properties.setProperty(Properties.java:230)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$asProperties$1(JDBCOptions.scala:54)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:54)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"newtable\"\n",
    "\n",
    "#filteringtable\n",
    "filter_condition = \"1=1 LIMIT 10000\"\n",
    "\n",
    "df3 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", f\"(SELECT * FROM {table_name} WHERE {filter_condition}) AS filtered_table\") \\\n",
    "    .options(**properties) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "row_count = df3.count()\n",
    "print(f\"Number of rows read: {row_count}\") \n",
    "\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average seasonal prices for a dataset containing date and price information, while also assigning each date a season label based on the month then writing the final table into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+\n",
      "|Season|Average Seasonal Price|\n",
      "+------+----------------------+\n",
      "|Summer|    1.9878632610939173|\n",
      "|Spring|    1.8628108614232208|\n",
      "|Autumn|    1.8136273425499252|\n",
      "|Winter|    1.7541149382253833|\n",
      "+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df3.withColumn('month', F.month('date')) \\\n",
    "        .withColumn('Season', F.when(F.col('month').between(3, 5), 'Spring')\n",
    "                               .when(F.col('month').between(6, 8), 'Summer')\n",
    "                               .when(F.col('month').between(9, 11), 'Autumn')\n",
    "                               .otherwise('Winter'))\n",
    "\n",
    "window_spec = Window.partitionBy('Season')\n",
    "\n",
    "seasonal_avg_prices = df.withColumn('Average Seasonal Price',\n",
    "                                    F.avg('Price').over(window_spec))\n",
    "\n",
    "seasonal_avg_prices = seasonal_avg_prices.dropDuplicates(['Season'])\n",
    "\n",
    "seasonal_avg_prices = seasonal_avg_prices.orderBy(F.col(\"Average Seasonal Price\").desc())\n",
    "\n",
    "seasonal_avg_prices.select(\"Season\", \"Average Seasonal Price\").show()\n",
    "\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"Seasontable\"\n",
    "\n",
    "columns_to_insert = (\"Season\",\"Average Seasonal Price\")\n",
    "\n",
    "seasonal_avg_prices.select(*columns_to_insert).write.jdbc(url=jdbc_url, table=table_name, mode=\"Overwrite\", properties=properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Find the distance between the locations with the minimum and maximum prices in a dataset then Writing thr resulting table into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Id='43099', Fuel_station_manager='GERACE GUSTAVO', Petrol_company='Agip Eni', Type='Stradale', Station_name='ENI', City=\"REGGIO NELL'EMILIA\", Latitude='44.69244033453445', Longitudine='10.648039201132178', isSelf=0, Price=1.209, Date=datetime.datetime(2022, 4, 28, 17, 27, 15))\n",
      "Row(Id='52927', Fuel_station_manager='ENERGY', Petrol_company='Pompe Bianche', Type='Stradale', Station_name='ENERGY', City='SAN DEMETRIO CORONE', Latitude='39.57206659091401', Longitudine='16.365120283739316', isSelf=1, Price=4.0, Date=datetime.datetime(2022, 10, 24, 10, 13, 52))\n",
      "+-----------------+\n",
      "|      Distance_km|\n",
      "+-----------------+\n",
      "|738.8053589909481|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_price_row = df3.orderBy(F.col(\"Price\")).first()\n",
    "max_price_row = df3.orderBy(F.col(\"Price\").desc()).first()\n",
    "\n",
    "print(min_price_row)\n",
    "print(max_price_row)\n",
    "\n",
    "#checking if min_price_row and max_price_row are not None\n",
    "if min_price_row is not None and max_price_row is not None:\n",
    "    min_latitude = float(min_price_row[\"Latitude\"])\n",
    "    min_longitude = float(min_price_row[\"Longitudine\"])\n",
    "    max_latitude = float(max_price_row[\"Latitude\"])\n",
    "    max_longitude = float(max_price_row[\"Longitudine\"])\n",
    "\n",
    "    min_latitude_rad = F.radians(F.lit(min_latitude)).cast(\"double\")\n",
    "    min_longitude_rad = F.radians(F.lit(min_longitude)).cast(\"double\")\n",
    "    max_latitude_rad = F.radians(F.lit(max_latitude)).cast(\"double\")\n",
    "    max_longitude_rad = F.radians(F.lit(max_longitude)).cast(\"double\")\n",
    "\n",
    "    distance_km = F.acos(\n",
    "    F.sin(min_latitude_rad) * F.sin(max_latitude_rad) +\n",
    "    F.cos(min_latitude_rad) * F.cos(max_latitude_rad) *\n",
    "    F.cos(max_longitude_rad - min_longitude_rad)\n",
    "    ).cast(\"double\") * 6371.0\n",
    "\n",
    "    df_with_distance = df3.withColumn(\"Distance_km\", distance_km)\n",
    "    df_with_distance.select(\"Distance_km\").distinct().show()\n",
    "else:\n",
    "    print(\"No data found to calculate minimum and maximum prices.\")\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\":pg_user,\n",
    "    \"password\": pg_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"Distance\"\n",
    "\n",
    "\n",
    "\n",
    "df_with_distance.write.jdbc(url=jdbc_url, table=table_name, mode=\"append\", properties=properties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average prices for each day of the month then presents the results in a pivot table then Writing the resulting table into Postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|day_of_month|           January|          February|             April|               May|              June|            August|         September|           October|          November|          December|\n",
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|          31|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|1.6961600719424508|\n",
      "|          28|               0.0|               0.0|1.8422384105960277|               0.0|2.1061851851851823|               0.0|               0.0|               0.0|1.6804042553191492|               0.0|\n",
      "|          12|               0.0|1.7942195945945962|               0.0|               0.0|               0.0|               0.0|1.7773640661938541|               0.0|               0.0|               0.0|\n",
      "|          22|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|             1.598|\n",
      "|          13|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|1.7679227799227766|               0.0|1.7227822966507176|\n",
      "|           6|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0| 2.109975903614456|               0.0|\n",
      "|           5|               0.0|               0.0|               0.0|               0.0|1.8714037656903755|               0.0|               0.0|               0.0|               0.0|               0.0|\n",
      "|          19|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|1.7247537437603966|               0.0|\n",
      "|          17|               0.0|               0.0|               0.0|               0.0|2.1347403598971706|               0.0|               0.0|               0.0|               0.0|               0.0|\n",
      "|           4|1.7937563025210062|               0.0|1.8594004424778765|               0.0|               0.0|1.8577222222222234|               0.0|               0.0|               0.0|               0.0|\n",
      "|          23|               0.0|               0.0|               0.0| 1.939681917211331|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|\n",
      "|          24|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|1.7059389978213506|               0.0|               0.0|\n",
      "|          21|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0| 1.713459154929576|\n",
      "|          14|               0.0|               0.0|               0.0|1.9009875776397531|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|\n",
      "|          18|               0.0|               0.0|1.8225202593192877|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|\n",
      "+------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df3.withColumn(\"day_of_month\", F.dayofmonth(\"Date\"))\n",
    "df = df.withColumn(\"month\", F.month(\"Date\"))\n",
    "\n",
    "\n",
    "day_pivot_table = df.groupBy(\"day_of_month\").pivot(\"month\").agg(F.avg(\"Price\"))\n",
    "\n",
    "# Define a list of month names\n",
    "month_names = [\n",
    "    \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "    \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(1, 13):\n",
    "    month_name = month_names[i - 1]\n",
    "    day_pivot_table = day_pivot_table.withColumnRenamed(str(i), month_name)\n",
    "\n",
    "\n",
    "day_pivot_table = day_pivot_table.fillna(0)\n",
    "\n",
    "day_pivot_table.show()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "properties = {\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"question3\"\n",
    "\n",
    "\n",
    "day_pivot_table.write.jdbc(url=jdbc_url, table=table_name, mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
